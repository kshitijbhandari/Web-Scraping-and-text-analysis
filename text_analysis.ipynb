{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "749fc654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kshit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a6eb1790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder_path = 'stopwords'\n",
    "\n",
    "# Get the list of files in the folder\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "all_words = []\n",
    "\n",
    "# Read each text file and extract words from each line\n",
    "for filename in file_list:\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                words_in_line = line.strip().split()\n",
    "                all_words.extend(words_in_line)\n",
    "\n",
    "all_words = [word for word in all_words if '|' not in word]\n",
    "all_words = [word for word in all_words if not word.startswith(('http', 'www.','Surnames'))]\n",
    "all_words = [word for word in all_words if '(' not in word and ')' not in word]\n",
    "# Print the list of all words\n",
    "# print(all_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "59cd1c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14231"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "66b265c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'MasterDictionary'\n",
    "\n",
    "\n",
    "\n",
    "positive_words = []\n",
    "negative_words = []\n",
    "\n",
    "# Read words from the first file and store them in positive_list\n",
    "file1_path = os.path.join(folder_path, 'positive-words.txt')\n",
    "with open(file1_path, 'r') as file1:\n",
    "    for line in file1:\n",
    "        words = line.strip().split()\n",
    "        positive_words.extend(words)\n",
    "\n",
    "# Read words from the second file and store them in negative_list\n",
    "file2_path = os.path.join(folder_path, 'negative-words.txt')\n",
    "with open(file2_path, 'r') as file2:\n",
    "    for line in file2:\n",
    "        words = line.strip().split()\n",
    "        negative_words.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "f382957b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "5192a076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of positive and negative list is 2006 , 4783\n"
     ]
    }
   ],
   "source": [
    "print('length of positive and negative list is', len(positive_words),',',len(negative_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "80e271bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = 'scraped_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca85ee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_neg_pola_subj_avglen(filename):\n",
    "    \n",
    "#     master_dict ={}\n",
    "    positive_list = []\n",
    "    negative_list = []\n",
    "    \n",
    "    file_path = os.path.join(folder_name, filename)\n",
    "    with open(file_path,'r',encoding='utf-8') as file:\n",
    "        corpus = file.read()\n",
    "        \n",
    "    corpus = corpus.lower()\n",
    "    \n",
    "    corpus_for_avglen = re.sub('[^A-Za-z0-9.]+', ' ', corpus)\n",
    "    sentences = nltk.sent_tokenize(corpus_for_avglen)\n",
    "\n",
    "    corpus = re.sub('[^A-Za-z0-9]+', ' ', corpus)\n",
    "\n",
    "    token_list = nltk.tokenize.word_tokenize(corpus)\n",
    "    \n",
    "    for word in token_list:\n",
    "        if word in positive_words:\n",
    "            positive_list.append(word)\n",
    "        elif word in negative_words:\n",
    "            negative_list.append(word)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    positive_score = len(positive_list)\n",
    "    negative_score = len(negative_list)\n",
    "    \n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "    \n",
    "    subjectivity_score = (positive_score + negative_score) / (len(token_list)+0.000001)\n",
    "    \n",
    "    avg_sent_len = len(token_list)/ len(sentences)\n",
    "    \n",
    "    vowels = \"aeiouy\"\n",
    "    \n",
    "#     print (sentence_word_counts)\n",
    "    return positive_score, negative_score, polarity_score, subjectivity_score, avg_sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a24be979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_fog_avgwords_complexwords_syllable(filename):\n",
    "    \n",
    "    file_path = os.path.join(folder_name, filename)\n",
    "    with open(file_path,'r',encoding='utf-8') as file:\n",
    "        corpus = file.read()\n",
    "        \n",
    "    corpus = corpus.lower()\n",
    "    \n",
    "    corpus = re.sub('[^A-Za-z0-9]+', ' ', corpus)\n",
    "#     sentences = nltk.sent_tokenize(corp)\n",
    "    token_list = nltk.tokenize.word_tokenize(corpus)\n",
    "    \n",
    "    vowels = 'aeiou'\n",
    "    syllable_per_word = []\n",
    "    complex_count = 0\n",
    "    for word in token_list:\n",
    "        if word.endswith('es') or word.endswith('ed'):\n",
    "            word = word[:-2]\n",
    "        else:\n",
    "            word = word\n",
    "        count = 0\n",
    "        for letter in word: \n",
    "            if letter in vowels:\n",
    "                count+=1\n",
    "            else:\n",
    "                pass\n",
    "        num_vowels = count\n",
    "        \n",
    "        syllable_per_word.append(num_vowels)\n",
    "        if count>=2:\n",
    "            complex_count+=1\n",
    "    complex_percent = (complex_count/len(token_list))*100   \n",
    "    \n",
    "    a,b,c,d,avg_sent_len = pos_neg_pola_subj_avglen(filename)\n",
    "    \n",
    "    avg_syllable_per_word = np.mean(syllable_per_word)  \n",
    "    \n",
    "    fog_index = 0.4 * (avg_sent_len + complex_percent)\n",
    "    \n",
    "    return complex_percent, fog_index, avg_sent_len, complex_count, avg_syllable_per_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bed48c",
   "metadata": {},
   "source": [
    "While finding syllable_per_word, I think I need to calculate avg syllable_per_word. syllable_per_word will be a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f9747a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcount_avgwordlen(filename):\n",
    "\n",
    "    file_path = os.path.join(folder_name, filename)\n",
    "#     print(file_path)\n",
    "    with open(file_path,'r',encoding='utf-8') as file:\n",
    "        corpus = file.read()\n",
    "        \n",
    "    corpus = corpus.lower()\n",
    "    \n",
    "    corpus = re.sub('[^A-Za-z0-9]+', ' ', corpus)\n",
    "    token_list = nltk.tokenize.word_tokenize(corpus)\n",
    "    \n",
    "    total_word_length = sum(len(word) for word in token_list)\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    filtered_words = [word for word in token_list if word not in stop_words]\n",
    "    filtered_corpus = \" \".join(filtered_words)\n",
    "    \n",
    "    final_token_list = nltk.tokenize.word_tokenize(filtered_corpus)\n",
    "    word_count = len(final_token_list)\n",
    "    \n",
    "    avg_word_len = total_word_length / len(token_list)\n",
    "    \n",
    "    return word_count, avg_word_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "787fd999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def personal_pronoun (filename):\n",
    "    \n",
    "    file_path = os.path.join(folder_name, filename)\n",
    "#     print(file_path)\n",
    "    with open(file_path,'r',encoding='utf-8') as file:\n",
    "        corpus = file.read()  \n",
    "    \n",
    "    token_upper = nltk.tokenize.word_tokenize(corpus)\n",
    "    \n",
    "    US_count = token_upper.count('US')\n",
    "#     print(US_count)\n",
    "    \n",
    "    corpus = corpus.lower()    \n",
    "    corpus = re.sub('[^A-Za-z0-9]+', ' ', corpus)\n",
    "\n",
    "    token_list = nltk.tokenize.word_tokenize(corpus)\n",
    "\n",
    "\n",
    "    target_words = ['i', 'we', 'my', 'ours', 'us']\n",
    "\n",
    "    # Initialize a Counter to count occurrences of each target word\n",
    "    word_counter = Counter()\n",
    "\n",
    "    for word in token_list:\n",
    "        if word in target_words:\n",
    "            word_counter[word] += 1\n",
    "\n",
    "    # Special handling for 'US' to avoid counting it as a pronoun\n",
    "    country_word = 'us'\n",
    "    word_counter[country_word] = word_counter[country_word] - US_count\n",
    "    personal_pronoun_count = sum(word_counter.values())\n",
    "\n",
    "    return personal_pronoun_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709a24ce",
   "metadata": {},
   "source": [
    "positive_score, negative_score, polarity_score, subjectivity_score, avg_sent_len       ##1,2,3,4,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f7d11bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_score = []; negative_score= []; polarity_score = []; subjectivity_score =[]; avg_sent_len =[]\n",
    "for file in text_files:\n",
    "    a,b,c,d,e = pos_neg_pola_subj_avglen(file)\n",
    "    positive_score.append(a)\n",
    "    negative_score.append(b)\n",
    "    polarity_score.append(c)\n",
    "    subjectivity_score.append(d)\n",
    "    avg_sent_len.append(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fc07ad",
   "metadata": {},
   "source": [
    "complex_fog_avgwords_complexwords_syllable    ## 6,7,8,9,11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "bd7982ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_compex_words = []; fog_index= []; avg_words_sent = []; complex_words_count =[]; syllable_per_word =[]\n",
    "for file in text_files:\n",
    "    f,g,h,i,k = complex_fog_avgwords_complexwords_syllable(file)\n",
    "    percent_compex_words.append(a)\n",
    "    fog_index.append(b)\n",
    "    avg_words_sent.append(c)\n",
    "    complex_words_count.append(d)\n",
    "    syllable_per_word.append(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13738c9",
   "metadata": {},
   "source": [
    "wordcount_avgwordlen   ##10,13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "97f51aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = [] ; avg_word_len = []\n",
    "for file in text_files:\n",
    "    j, m = wordcount_avgwordlen(file)\n",
    "    word_count.append(j)\n",
    "    avg_word_len.append(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bcd261",
   "metadata": {},
   "source": [
    "personal_pronoun    ##12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "0df7732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "personal_pronoun_count = []\n",
    "for file in text_files:\n",
    "    l = personal_pronoun (file)\n",
    "    personal_pronoun_count.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "d1cfa8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_val(column):\n",
    "    column.insert(7,'-'); column.insert(20,'-'); column.insert(107,'-')\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "41307577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "output = pd.read_excel('Output Data Structure.xlsx')\n",
    "\n",
    "output['POSITIVE SCORE'] = insert_val(positive_score)\n",
    "output['NEGATIVE SCORE'] = insert_val(negative_score)\n",
    "output['POLARITY SCORE'] = insert_val(polarity_score)\n",
    "output['SUBJECTIVITY SCORE'] = insert_val(subjectivity_score)\n",
    "output['AVG SENTENCE LENGTH'] = insert_val(avg_sent_len)\n",
    "output['PERCENTAGE OF COMPLEX WORDS'] = insert_val(percent_compex_words)\n",
    "output['FOG INDEX'] = insert_val(fog_index)\n",
    "output['AVG NUMBER OF WORDS PER SENTENCE'] = insert_val(avg_words_sent)\n",
    "output['COMPLEX WORD COUNT'] = insert_val(complex_words_count)\n",
    "output['WORD COUNT'] = insert_val(word_count)\n",
    "output['SYLLABLE PER WORD'] = insert_val(syllable_per_word)\n",
    "output['PERSONAL PRONOUNS'] = insert_val(personal_pronoun_count)\n",
    "output['AVG WORD LENGTH'] = insert_val(avg_word_len)\n",
    "\n",
    "\n",
    "# Save the updated DataFrame back to the Excel file\n",
    "output.to_excel('output_new.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
